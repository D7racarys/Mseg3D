# 想法
1. 用Hub画布做跨模态特征补全
2. 加入异构参数思想，用不同的1*1模块聚合多尺度特征

### 实验步骤
1. 新增cmc文件，用于FOV内外点的投影（似乎没必要，points_cuv中有要用的数据）
2. 要大改，先建git仓库
3. 先找GF-Phase是在哪里做的，把这个阶段更换为Hub融合？不过也好像可以直接改跨模态对齐
4. 先尝试保留GF-Phase，将原来的特征补全替换为cmnext，要更改网络架构和损失更新
5. 具体做法是cmnext融合后生成一个Hub特征图，然后把LiDAR投影到这个Hub内，FOV内直接用对应的融合特征，FOV外的点还需要学一个映射器自适应的把点嵌入到这个Hub当中
6. 关于这一点有很多做法：
    1. cmnext融合取代GF-phase
    2. 仅仅只替换跨模态特征补全部分
    3. 前面全部用cmnext（包括特征提取等部分）然后接一个SF-Phase（比较简单，可以先做）
        前者输出[B, C, H, W]即[1, 512, 256, 256]，这似乎是一个图像的融合特征图
        后者输入为lidar和RGB的两个语义向量（这部分可以通过mseg3d的backbone进行提取，然后通过SFAM）
            以及一个GF-Phase的向量[n, fused_chs]
    4. 要记得加上参数异构的想法

### 实验中的困惑
1. 怎么改网络架构
2. 我在想一个问题，我希望使用cmnext的方式生成一个融合的2d特征图（在decode_head之前），然后将LiDAR投影到这个特征图上，对于那些能够投影到特征图上的点的相机特征则对该点所处的特征图的位置做双线性插值，对于那些不能投影的则另外学习一个映射器Gp，以使LiDAR点能够嵌入到这个特征图空间中。以上是我希望实现的跨模态特征补全的机制，我希望使用这个机制替换原本的mseg3d中的跨模态补全机制，其余部分不变，那么整个训练过程中，在这个跨模态补全机制中，应该是对哪个部分进行学习，是对cmnext这个网络进行学习，还是我提到的这个映射器Gp?
3. 环境这边出了问题，mseg3d和cmnext的环境不兼容，主要是这两个环境装不到一起的话就做不了接下来的实验验证，总之还是先装环境，尝试都升级到3.8.12和cu113（对齐cmnext）可能有一些mseg3d里面的包要升级，要去找找文档或者网上的经验，在做的过程中也记录一下关键步骤，如果成功的话，之后也能在185上复现（不过之前还是卡在一个cuDNN的查找过程）
    cu113似乎是cpu版本，试试cu111

### 学习到的新知识
1. __call__通常在两种情况下调用：
    1. 第一种是obj(params)，将类的实例当作函数调用时调用__call__;
    2. 第二种是数据流经pipeline就会自动触发类中的__call__实现transform。
    和__init__不同，__init__只在最初注册时调用，准备好网络及相关参数，__call__在每次数据流过时都进行调用，可能多次调用
2. points_cuv: (n0+n1+...n_{b-1}, 4)中的n0, n1, n2由来
    LiDAR每帧扫描一次，得到一个样本，其中ni表示每帧扫描到的点数，即一个样本中点的数量

